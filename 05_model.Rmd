# Model

"Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. ... But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. 
- Garrett Grolemund & Hadley Wickham, *R for Data Science*"

[`{base}`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/base-package.html)

- `I()`
  - Use when raising a variable to an exponent in order to evaluate the value as a mathematical expression rather than as an interaction variable.
- `mean()`
  - Use in modeling, such as classification trees, to compute the accuracy of predicted values; see example below.
- `sample()`
- `set.seed()`
- `summary()`
  - Pass the model as the data argument.

[`{bayesplot}`](https://mc-stan.org/bayesplot/)

[`{brms}`](https://paul-buerkner.github.io/brms/index.html)

- See also `{bayesplot}`, `{tidybayes}`.

`{broom}`

- `augment()`
  - Augment data with information from an object.
- `glance()`
  - Construct a single row summary of a model, fit, or other object.
- `tidy()`
  - Turn an object into a tidy tibble.

[`{car}`](https://cran.r-project.org/web/packages/car/index.html)

`{caret}`

`{class}`

- `knn()`

`{cluster}`

- `pam()`
  - Useful for silhouette analysis; similar but not identical to `stats::kmeans()`.

`{dplyr}`

- `sample_n()`

`{gbm}`

[`{infer}`](https://cran.r-project.org/package=infer)

- `generate()`
  - Set `type = "permute"` when paired with `hypothesize(null = "independence")` and `type = "bootstrap"` or `type = "simulate"` when paired with `hypothesize(null = "point")`.

[`{MatchIt}`](https://cran.r-project.org/package=MatchIt)

- `match.data()`
- `matchit()`

`{mgcv}`

- `gam()`
  - Generalized additive models (GAMs) with integrated smoothness estimation.
  - Use `plot()` on a GAM object to view what the function learned from the data.
- `s()`
  - Indicates which of the explanatory variables should be considered non-linearly.
  - Use on continuous variables and avoid using on categorical variables.

`{mixtools}`

`{naivebayes}`

- `naive_bayes()`

[`{performance}`](https://cran.r-project.org/package=performance)

`{pROC}`

- `auc()`
- `roc()`

`{randomForest}`

See `{ranger}`.

- `randomForest()`

`{ranger}`

See `{randomForest}`.

- `ranger()`

`{rpart}`

- `rpart()`

`{rpart.plot}`

- `rpart.plot()`

`{rsample}`

`{sigr}`

- `wrapChiSqTest()`
  - For Pseudo-R^2:
    - On training data, use `wrapChiSqTest(model)`. For test data, use `wrapChiSqTest(prediction_column_name, outcome_column_name, target_value)`.


[`{stats}`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/stats-package.html)

- See `{infer}` for easier-to-use hypothesis testing functions.

- `cutree()`
  - Pair with `stats::hclust()`.
- `as.formula()`
  - Use to define an object to pass as a formula into a model (e.g., `fmla <- as.formula("unemployment ~ population"), lm(fmla, data)`)
- `coef()`
  - Extract model coefficients.
- `cor()`
- `cov()`
- `cov2cor()`
  - Scale a covariance matrix into a correlation matrix.
- `cutree()`
  - Pair with `stats::hclust()`.
- `df.residual()`
  - Get the residual degrees of freedom.
- `dist()`
- Distributions
  - Binomial (`dbinom()`)
  - Exponential (`dexp()`)
  - Gamma (`dgamma()`)
  - Geometric (`dgeom()`)
  - Log-normal (`dlnorm()`)
  - Normal (`dnorm()`)
  - Poisson (`dpois()`)
  - Uniform (`dunif()`)
- `glm()`
  - For `family = Gamma`, consider using `(link = "log")` rather than the default. Use `expm1()` on the coefficients to tranform them into a percentage, which is easier to interpret than a logged coefficient value.
- `hclust()`
- `kmeans()`
- `lm()`
  - Fit linear models.
- `model.matrix()`
  - Use to create dummy variables from categorical data.
  - See `{vtreat}` functions.
- `p.adjust()`
  - Adjust p-values for multiple comparisons.
- `pairwise.t.test()`
- `prcomp()`
  - Principal Component Analysis.
- `predict()`
  - Model predictions.
  - For `lm` class objects, predict new values using `predict(model, newdata)`.
  - For `glm` class objects, predict new values using `predict(model, newdata, type = "response")` (`predict()` returns the log-odds of the event, instead of the probability, if `type = "response"` is not included).
  - For GAM, `predict()` returns a matrix so use `as.numeric()` to convert the matrix to a vector if adding to a data set (such as for plotting the predictions versus the actual outcomes).
  - Access the y values of the model using `predict(model, type = "terms")`.
- `prop.test()`
  - See `infer::prop_test()`.
- `residuals()`
  - Extract model residuals.
- `step()`
  - Step-wise regression.
- `t.test()`
  - See `infer::t_test()`.

[`{tidybayes}`](https://mjskay.github.io/tidybayes/)

`{tidyr}`

- `nest()`
  - Nest repeated values in a list-variable.
  - Helpful when separating a data frame in preparation to model the data for each grouping.
  
`{vtreat}`

Compared to `stats::model.matrix()`, `{vtreat}` functions allow for handling new categorical variables in the test data not seen in the training data.

- `designTreatmentsZ()`
- `kWayCrossValidation()`
- `prepare()`
- `splitPlan()`
  - Use to create a cross-validation plan.

`{WVPlots}`

- `GainCurvePlot()`
  - Use to evaluate logistic regression models.

`{xgboost}`

- `xgb.cv()`
  - Use `xgb.cv()$evaluation_log` to find the number of trees that minimizes RMSE (input that number as the `nrounds` argument in `xgboost()`).

### Examples

`base::mean()`

```{r, eval=FALSE}
# Classification Trees: Compute accuracy of predictions on the test data set:
mean(loans_test$outcome == loans_test$prediction)
```
